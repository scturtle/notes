#+TITLE: Note of cs231n
#+HTML_HEAD_EXTRA: <link rel="stylesheet" type="text/css" href="org.css" />
#+OPTIONS: toc:nil num:3 H:4 ^:nil pri:t

#+TOC: headlines 2

* Classification
L1 distance: $d_1 (I_1, I_2) = \sum_{p} \left| I^p_1 - I^p_2 \right|$

L2 distance: $d_2 (I_1, I_2) = \sqrt{\sum_{p} \left( I^p_1 - I^p_2 \right)^2}$

k-Nearest Neighbor (kNN) classifier

[[http://homepage.tudelft.nl/19j49/t-SNE.html][t-SNE]]

If your data is very high-dimensional, consider using a dimensionality reduction technique
such as PCA ([[https://www.wikiwand.com/en/Principal_component_analysis][wiki ref]], [[http://cs229.stanford.edu/notes/cs229-notes10.pdf][CS229ref]], [[http://cs229.stanford.edu/notes/cs229-notes10.pdf][blog ref]]) or even [[http://scikit-learn.org/stable/modules/random_projection.html][Random Projections]].

If your kNN classifier is running too long, consider using an Approximate Nearest Neighbor library
(e.g. [[http://www.cs.ubc.ca/research/flann/][FLANN]]) to accelerate the retrieval (at cost of some accuracy).

Ref:

[[http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf][A Few Useful Things to Know about Machine Learning]]

[[http://people.csail.mit.edu/torralba/shortCourseRLOC/index.html][Recognizing and Learning Object Categories]], ICCV 2005


* Linear Cliassify

Linear classifier:

\[f(x_i, W, b) =  W x_i + b\]

** Multiclass Support Vector Machine

the score for the j-th class is the j-th element:

\[s_j=f(x_i,W)_j\]

loss for the i-th example:

\[L_i = \sum_{j\neq y_i} \max(0, s_j - s_{y_i} + \Delta)\]

vectorlize:

\[f(x_i; W) =  W x_i\]

\[L_i = \sum_{j\neq y_i} \max(0, w_j^T x_i - w_{y_i}^T x_i + \Delta)\]


In summary, the SVM loss function wants the score of the correct class
$y_i$ to be larger than the incorrect class scores by at least by $\Delta$.

The setting of $\Delta$ (1.0 is fine) is in some sense meaningless because the weights can shrink.

The threshold at zero $max(0,-)$ function is often called the *hinge loss*.

** Softmax classifier

generalization of binary Logistic Regression classifier to multiple classes

output normalized class probabilities and also has a probabilistic interpretation

cross-entropy loss:

\[L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right) \hspace{0.1in} \text{or equivalently} \hspace{0.1in} L_i = -f_{y_i} + \log\sum_j e^{f_j}\]

$f_i$ is the i-th element of the vector of class scores $f$

softmax function:

\[f_j(z) = \frac{e^{z_j}}{\sum_k e^{z_k}}\]

Maximum Likelihood Estimation (MLE)

Maximum a posteriori (MAP) (Gaussian prior?)

The SVM classifier uses the hinge loss, or also sometimes called the max-margin loss. The Softmax classifier uses the cross-entropy loss.

ref:
[[http://arxiv.org/abs/1306.0239][Deep Learning using Linear Support Vector Machines]]


* Optimization

Gradient Descent

backpropagation


* Neural Networks

** Activation functions

sigmoid: \[\sigma(x) = 1/(1+e^{-x})\]

- (-) Sigmoids saturate and kill gradients.
- (-) Sigmoid outputs are not zero-centered.

tanh non-linearity is always preferred to the sigmoid nonlinearity.

ReLU: \[f(x) = \max(0, x)\]

- (+) Simple and fast.
- (-) Can be fragile during training and can “die”.

Leaky ReLU : \[f(x) = \mathbb{1} (x < 0) (\alpha x) + \mathbb{1} (x>=0) (x)\]

-  [[http://arxiv.org/abs/1502.01852][Delving Deep into Rectifiers]], Kaiming He et al., 2015
- attempt to fix the “dying ReLU” problem

Maxout: \[\max(w_1^Tx+b_1, w_2^Tx + b_2)\]

- by [[http://www-etud.iro.umontreal.ca/~goodfeli/maxout.html][Goodfellow et al.]] that generalizes the ReLU and its leaky version
- (-) doubles the number of parameters

TLDR:
- *Use the ReLU non-linearity*, be careful with your learning rates and possibly monitor the fraction of “dead” units in a network.
- If this concerns you, give Leaky ReLU or Maxout a try.
- Never use sigmoid. Try tanh, but expect it to work worse than ReLU/Maxout.


** PCA

#+BEGIN_SRC python
# Assume input data matrix X of size [N x D]
X -= np.mean(X, axis = 0) # zero-center the data (important)
cov = np.dot(X.T, X) / X.shape[0] # get the data covariance matrix
U,S,V = np.linalg.svd(cov) #  SVD factorization
#+END_SRC

Columns of U are the eigenvectors and S is a 1-D array of the singular values
(which are equal to the eigenvalues squared).

projection and dimensionality reduction:
#+BEGIN_SRC python
Xrot = np.dot(X, U) # decorrelate the data
Xrot_reduced = np.dot(X, U[:,:100]) # Xrot_reduced becomes [N x 100]
#+END_SRC


** Whitening

The whitening operation takes the data in the eigenbasis and
divides every dimension by the eigenvalue to normalize the scale.

#+BEGIN_SRC python
Xwhite = Xrot / np.sqrt(S + 1e-5)
#+END_SRC


** Weight initialization

The recommended heuristic is to initialize each neuron’s weight vector as:
~w = np.random.randn(n) / sqrt(n)~, where n is the number of its inputs.

Another choice: $\text{Var}(w) = 2/(n_{in} + n_{out})$.

[[http://arxiv-web3.library.cornell.edu/abs/1502.01852][Recommend]]: ReLU unit and ~w = np.random.randn(n) * sqrt(2.0/n)~.

[[http://arxiv.org/abs/1502.03167][Batch Normalization]]: explicitly forcing the activations throughout a network
to take on a unit gaussian distribution at the beginning of the training.
Insert the BatchNorm layer immediately after fully connected layers
(or convolutional layers), and before non-linearities.


** Regularization

*L2 regularization* encouraging the network to use all of its inputs a little
rather that some of its inputs a lot.

*L1 regularization* leads the weight vectors to become sparse during optimization
 (i.e. very close to exactly zero).
Neurons end up using only a sparse subset of their most important inputs and
 become nearly invariant to the “noisy” inputs.

*Max norm constraints* enforce an absolute upper bound on the magnitude
 of the weight vector. $\Vert \vec{w} \Vert_2 < c$, c are on orders of 3 or 4.

[[http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf][Dropout]] is implemented by only keeping a neuron active with some probability
 $p$ (a hyperparameter), or setting it to zero otherwise.
In the predict function we are not dropping anymore but performing a scaling.
inverted dropout performs the scaling at train time,
 leaving the forward pass at test time untouched.

*In practice*: It is most common to use a single, global L2 regularization strength
that is cross-validated.
It is also common to combine this with dropout applied after all layers.
The value of p=0.5.


** Loss functions

*** Classification

SVM:

\[L_i = \sum_{j\neq y_i} \max(0, f_j - f_{y_i} + 1)\]

Softmax classifier uses the cross-entropy loss:

\[L_i = -\log\left(\frac{e^{f_{y_i}}}{ \sum_j e^{f_j} }\right)\]

[[http://arxiv.org/pdf/1310.4546.pdf][Hierarchical Softmax]]


*** Attribute/Multiclass classification

Binary classifier for each class:

\[L_i = \sum_j \max(0, 1 - y_{ij} f_j)\]

Logistic regression classifier for every class
 and probability for class 1 is:

\[P(y = 1 \mid x; w, b) = \frac{1}{1 + e^{-(w^Tx +b)}} = \sigma (w^Tx + b)\]

The loss function then maximizes the log likelihood of this probability:

\[L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j))\]

Gradient on $f$:

\[\partial{L_i} / \partial{f_j} = y_{ij} - \sigma(f_j)\]


*** Regression

L2/L1.

Word of caution:

1. It is important to note that the L2 loss is much harder
 to optimize than a more stable loss such as Softmax.

2. Additionally, the L2 loss is less robust because
 outliers can introduce huge gradients.

3. When faced with a regression problem, first consider if it
 is absolutely inadequate to quantize the output into bins.

4. the L2 is more fragile and applying dropout in the network
 (especially in the layer right before the L2 loss) is not a great idea.

#+BEGIN_QUOTE
 When faced with a regression task, first consider if it is absolutely necessary.
 Instead, have a strong preference to discretizing your outputs to bins and
 perform classification over them whenever possible.
#+END_QUOTE


*** Structured prediction

The basic idea behind the structured SVM loss is to demand a margin
 between the correct structure $y_i$ and the highest-scoring incorrect structure.


** Summary

1. The recommended preprocessing is to center the data to have mean of zero,
 and normalize its scale to [-1, 1] along each feature.

2. Initialize the weights by drawing them from a gaussian distribution
 with standard deviation of $\sqrt{2/n}$, where $n$ is the number of inputs to the neuron.
 E.g. in numpy: ~w = np.random.randn(n) * sqrt(2.0/n)~.

3. Use L2 regularization and dropout (the inverted version).

4. Use batch normalization.


** Gradient check

\[\frac{df(x)}{dx} = \frac{f(x + h) - f(x - h)}{2h}\]

\[\frac{\mid f'_a - f'_n \mid}{\max(\mid f'_a \mid, \mid f'_n \mid)}\]

[[http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html][“What Every Computer Scientist Should Know About Floating-Point Arithmetic”]]


*Kinks* refer to non-differentiable parts of an objective function.

It is best to use a short burn-in time during which the network is allowed to
 learn and perform the gradient check after the loss starts to go down.

It is recommended to turn off regularization and check the data loss alone first,
 and then the regularization term second and independently.

Remember to turn off any non-deterministic effects in the network,
 such as dropout, random data augmentations, etc.


** Ratio of weights:updates

#+BEGIN_SRC python
# assume parameter vector W and its gradient vector dW
param_scale = np.linalg.norm(W.ravel())
update = -learning_rate*dW # simple SGD update
update_scale = np.linalg.norm(update.ravel())
W += update # the actual update
print update_scale / param_scale # want ~1e-3
#+END_SRC


** Parameter updates

*** Momentum update

#+BEGIN_SRC python
v = mu * v - learning_rate * dx # integrate velocity
x += v # integrate position
#+END_SRC

A typical setting is to start with momentum of about 0.5 and
 anneal it to 0.99 or so over multiple epochs.


*** Nesterov’s Accelerated Momentum (NAG)

#+BEGIN_SRC python
x_ahead = x + mu * v
# evaluate dx_ahead (the gradient at x_ahead instead of at x)
v = mu * v - learning_rate * dx_ahead
x += v
#+END_SRC

Expressing the update in terms of x_ahead instead of x:

#+BEGIN_SRC python
v_prev = v # back this up
v = mu * v - learning_rate * dx # velocity update stays the same
x += -mu * v_prev + (1 + mu) * v # position update changes form
#+END_SRC


*** Annealing the learning rate

1. Step decay.
2. *Exponential decay*. has the mathematical form $\alpha = \alpha_0 e^{-k t}$,
 where $t$ is the iteration number/units of epochs.
3. *1/t decay* has the mathematical form $\alpha = \alpha_0 / (1 + k t)$.


*** Second order methods

Based on Newton’s method:

\[x \leftarrow x - [H f(x)]^{-1} \nabla f(x)\]

Here, $H f(x)$ is the Hessian matrix, which is a square matrix
 of second-order partial derivatives of the function.
The term $\nabla f(x)$ is the gradient vector, as seen in Gradient Descent.

[[http://en.wikipedia.org/wiki/Limited-memory_BFGS][L-BFGS]] uses the information in the gradients over time
 to form the approximation implicitly.
However it must be computed over the entire training set.

[[http://arxiv.org/abs/1311.2115][SFO]] algorithm strives to combine the advantages of SGD with advantages of L-BFGS.


*** Per-parameter adaptive learning rate methods


**** Adagrad

originally proposed by [[http://jmlr.org/papers/v12/duchi11a.html][Duchi et al.]].

#+BEGIN_SRC python
# Assume the gradient dx and parameter vector x
cache += dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
#+END_SRC

A downside of Adagrad is that in case of Deep Learning,
 the monotonic learning rate usually proves too aggressive and stops learning too early.


**** RMSprop

[[http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf][Slide 29 of Lecture 6]] of Geoff Hinton’s Coursera class.

#+BEGIN_SRC python
cache = decay_rate * cache + (1 - decay_rate) * dx**2
x += - learning_rate * dx / (np.sqrt(cache) + eps)
#+END_SRC

Here, ~decay_rate~ is a hyperparameter and typical values are [0.9, 0.99, 0.999].


**** [[http://arxiv.org/abs/1412.6980][Adam]]

A recently proposed update that looks a bit like RMSProp with momentum.
 The simplified update looks as follows:

#+BEGIN_SRC python
m = beta1*m + (1-beta1)*dx
v = beta2*v + (1-beta2)*(dx**2)
x += - learning_rate * m / (np.sqrt(v) + eps)
#+END_SRC

Recommended values in the paper are eps = 1e-8, beta1 = 0.9, beta2 = 0.999.

In practice Adam is currently *recommended as the default algorithm* to use,
 and often works slightly better than RMSProp.

However, it is often also worth trying /SGD+Nesterov Momentum/ as an alternative.

The full Adam update also includes a /bias correction/ mechanism.


** Hyperparameter optimization

Prefer one validation fold to cross-validation.

Search for hyperparameters on log scale.

Prefer random search to grid search.

Bayesian Hyperparameter Optimization: [[https://github.com/JasperSnoek/spearmint][Spearmint]], [[http://www.cs.ubc.ca/labs/beta/Projects/SMAC/][SMAC]], and [[http://jaberg.github.io/hyperopt/][Hyperopt]].
However, in practical settings with ConvNets it is still relatively
 difficult to beat random search in a carefully-chosen intervals.


** Model Ensembles

1. Same model, different initializations.
2. Top models discovered during cross-validation.
3. Different checkpoints of a single model.
  Taking different checkpoints of a single network over time
  (for example after every epoch).
  This works reasonably well in practice and is very cheap.
4. Running average of parameters during training.
  Maintain a second copy of the network’s weights in memory that
  maintains an exponentially decaying sum of previous weights during training.
  A cheap way of almost always getting an extra percent or two of performance.

Geoff Hinton on [[https://www.youtube.com/watch?v=EK61htlw8hY][“Dark Knowledge”]].

ref:
[[http://research.microsoft.com/pubs/192769/tricks-2012.pdf][SGD tips and tricks]] from Leon Bottou
[[http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf][Efficient BackProp]] (pdf) from Yann LeCun
[[http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf][Practical Recommendations for Gradient-Based Training of Deep Architectures]] from Yoshua Bengio


* Convolutional Networks

**  The Conv Layer

\[(W - F + 2P)/S + 1\]

- $W$: input volume size
- $F$: the receptive field size of the Conv Layer neurons
- $S$: the stride with which they are applied
- $P$: the amount of zero padding used

Notice that if all neurons in a single depth slice are using the same weight vector,
 then the forward pass of the CONV layer can in each depth slice be computed as a
 convolution of the neuron’s weights with the input volume (Hence the name: Convolutional Layer).

To summarize:

- input volume size: $W_1 \times W_1 \times D_1$
- parameters:
  - Number of filters $K$
  - their spatial extent $F$
  - the stride $S$
  - the amount of zero padding $P$
- output volume size: $W_2 \times W_2 \times D_2$:
  - $W_2 = (W_1 - F + 2P)/S + 1$
  - $D_2 = K$

The backward pass for a convolution operation (for both the data and the weights)
 is also a convolution (but with spatially-flipped filters).

[[https://arxiv.org/abs/1511.07122][Dilated convolutions]].


** The Pooling Layer

- input volume size: $W_1 \times W_1 \times D_1$
- parameters:
  - their spatial extent $F$
  - the stride $S$
- output volume size: $W_2 \times W_2 \times D_2$:
  - $W_2 = (W_1 - F)/S + 1$
  - $D_2 = D_1$

Average pooling was often used historically but has recently fallen out of favor
 compared to the max pooling operation, which has been shown to work better in practice.

The backward pass for a max(x, y) operation has a simple interpretation
 as only routing the gradient to the input that had the highest value in the forward pass.
 Hence, during the forward pass of a pooling layer it is common to keep track of
 the index of the max activation (sometimes also called the switches).

*Getting rid of pooling*: [[http://arxiv.org/abs/1412.6806][Striving for Simplicity: The All Convolutional Net]].
 To reduce the size of the representation they suggest using larger stride in CONV layer once in a while.
 Discarding pooling layers has also been found to be important in training good generative models,
 such as variational autoencoders (VAEs) or generative adversarial networks (GANs).
 It seems likely that future architectures will feature very few to no pooling layers.


** Normalization Layer

Normalization layers have since fallen out of favor because
 in practice their contribution has been shown to be minimal, if any.

For various types of normalizations, see the discussion in [[http://code.google.com/p/cuda-convnet/wiki/LayerParams#Local_response_normalization_layer_(same_map)][cuda-convnet library API]].


** FC->CONV conversion

The ability to convert an FC layer to a CONV layer is particularly useful in practice.
It turns out that this conversion allows us to “slide” the original ConvNet
 very efficiently across many spatial positions in a larger image, *in a single forward pass*.

It is common to resize an image to make it bigger, use a converted ConvNet
 to evaluate the class scores at many spatial positions and then average the class scores.


** ConvNet Architectures

~INPUT -> [[CONV -> RELU]*N -> POOL?]*M -> [FC -> RELU]*K -> FC~

Usually N <= 3 and K < 3.

Common ConvNet architectures:

- ~INPUT -> FC~, implements a linear classifier.
- ~INPUT -> CONV -> RELU -> FC~.
- ~INPUT -> [CONV -> RELU -> POOL]*2 -> FC -> RELU -> FC~.
- ~INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]*3 -> [FC -> RELU]*2 -> FC~.

Multiple stacked CONV layers can develop more complex features
 of the input volume before the destructive pooling operation.

Stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters
 allows us to express more powerful features of the input, and with fewer parameters.

As a practical disadvantage, we might need more memory to hold all the intermediate
 CONV layer results if we plan to do backpropagation.


** Layer Sizing Patterns

The input layer (that contains the image) should be divisible by 2 many times.
 Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.

The conv layers should be using small filters (e.g. 3x3 or at most 5x5),
 using a stride of $S=1$, and *crucially*, zero-pad.

It is only common to see bigger filter sizes (such as 7x7 or so) on the
 very first conv layer that is looking at the input image.

The most common setting for pooling layer is to use max-pooling
 with 2x2 receptive fields, and with a stride of 2.
 Another slightly less common setting is to use 3x3 receptive fields with a stride of 2.


** Case studies

[[http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf][LeNet]]. The first successful applications of ConvNet.

[[http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks][AlexNet]]. The first work that popularized ConvNet. Winner of ILSVRC challenge in 2012.
Featured Convolutional Layers *stacked* on top of each other.

[[http://arxiv.org/abs/1311.2901][ZFNet]]. The ILSVRC 2013 winner.

[[http://arxiv.org/abs/1409.4842][GoogLeNet]]. The ILSVRC 2014 winner.
 *Inception Module* that dramatically reduced the number of parameters in the network.
 *Average Pooling* instead of Fully Connected layers at the top of the ConvNet,
 eliminating a large amount of parameters.
 most recently [[http://arxiv.org/abs/1602.07261][Inception-v4]].

[[http://www.robots.ox.ac.uk/~vgg/research/very_deep/][VGGNet]]. Its main contribution was in showing that the depth of the network
 is a critical component for good performance.
 Their final best network contains *16 CONV/FC layers that only performs 3x3 convolutions and 2x2 pooling*.
 Their [[http://www.robots.ox.ac.uk/~vgg/research/very_deep/][pretrained model]] is available for plug and play use in Caffe.

[[http://arxiv.org/abs/1512.03385][ResNet]]. *State of the art* as of May 10, 2016. The winner of ILSVRC 2015.
 It features special skip connections and a heavy use of [[http://arxiv.org/abs/1502.03167][batch normalization]]. 
 Kaiming’s presentation ([[https://www.youtube.com/watch?v=1PGLj-uKT1w][video]], [[http://research.microsoft.com/en-us/um/people/kahe/ilsvrc15/ilsvrc2015_deep_residual_learning_kaiminghe.pdf][slides]]), and [[https://github.com/gcr/torch-residual-networks][reproduced in Torch]].
 Also Kaiming He et al. [[https://arxiv.org/abs/1603.05027][Identity Mappings in Deep Residual Networks]] (published March 2016).

[[https://github.com/soumith/convnet-benchmarks][Soumith benchmarks for CONV performance]].
